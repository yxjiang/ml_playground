{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "data_url = \"https://download.pytorch.org/tutorial/data.zip\"\n",
    "dir_path = \"./data\"\n",
    "zip_file_name = \"data.zip\"\n",
    "zip_file_path = os.path.join(dir_path, zip_file_name)\n",
    "\n",
    "if not os.path.exists(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open(zip_file_path, \"wb\") as f:\n",
    "  f.write(requests.get(data_url).content)\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path) as z:\n",
    "  z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/French.txt', 'data/names/Czech.txt', 'data/names/Portuguese.txt', 'data/names/Vietnamese.txt', 'data/names/Greek.txt', 'data/names/Spanish.txt', 'data/names/Italian.txt', 'data/names/Chinese.txt', 'data/names/German.txt', 'data/names/Irish.txt', 'data/names/Dutch.txt', 'data/names/Arabic.txt', 'data/names/Russian.txt', 'data/names/Scottish.txt', 'data/names/English.txt', 'data/names/Polish.txt', 'data/names/Japanese.txt', 'data/names/Korean.txt']\n",
      "Slusarski\n",
      "French 277\n",
      "Czech 519\n",
      "Portuguese 74\n",
      "Vietnamese 73\n",
      "Greek 203\n",
      "Spanish 298\n",
      "Italian 709\n",
      "Chinese 268\n",
      "German 724\n",
      "Irish 232\n",
      "Dutch 297\n",
      "Arabic 2000\n",
      "Russian 9408\n",
      "Scottish 100\n",
      "English 3668\n",
      "Polish 139\n",
      "Japanese 991\n",
      "Korean 94\n",
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "for category, lines in category_lines.items():\n",
    "  print('%s %d' % (category, len(lines)))\n",
    "\n",
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\n",
      "35\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "print(all_letters)\n",
    "print(letterToIndex(\"J\"))\n",
    "\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor(\"J\"))\n",
    "\n",
    "def lineToTensor(line):\n",
    "  tensor = torch.zeros(len(line), 1, n_letters)\n",
    "  for i, letter in enumerate(line):\n",
    "    tensor[i][0][letterToIndex(letter)] = 1\n",
    "  return tensor\n",
    "\n",
    "print(lineToTensor(\"Jiang\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "lstm = nn.LSTM(input_size=n_letters, hidden_size=n_hidden).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input1.shape: torch.Size([5, 1, 57])\n",
      "LSTM(57, 128)\n",
      "packed_input: PackedSequence(data=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]], device='cuda:0'), batch_sizes=tensor([2, 2, 2, 2, 2, 1, 1]), sorted_indices=tensor([1, 0], device='cuda:0'), unsorted_indices=tensor([1, 0], device='cuda:0'))\n",
      "output: tensor([[-0.2051,  0.2447, -0.1123,  ..., -0.1474,  0.2046, -0.0580],\n",
      "        [-0.1213,  0.4999, -0.0269,  ..., -0.3021,  0.1202, -0.2281],\n",
      "        [-0.1544,  0.0698, -0.0084,  ..., -0.1279,  0.2288,  0.0511],\n",
      "        ...,\n",
      "        [ 0.0166,  0.0504,  0.0363,  ..., -0.0186,  0.0292,  0.0469],\n",
      "        [-0.0437,  0.0307,  0.0693,  ..., -0.0156,  0.0576,  0.0850],\n",
      "        [-0.0101,  0.0129,  0.0699,  ...,  0.0007,  0.0547,  0.0749]],\n",
      "       device='cuda:0', grad_fn=<CudnnRnnBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "input1 = lineToTensor('Jiang').to(device)\n",
    "input2 = lineToTensor('William').to(device)\n",
    "\n",
    "print(\"input1.shape:\", input1.shape)\n",
    "# print(all_categories[output.topk(1)[1].item()])\n",
    "\n",
    "h0 = torch.randn(1, 2, n_hidden).to(device)  # num_layers*num_directions, batch, hidden_size\n",
    "c0 = torch.randn(1, 2, n_hidden).to(device)  # num_layers*num_directions, batch, hidden_size\n",
    "print(lstm)\n",
    "packed_input = pack_sequence([input1.squeeze(1), input2.squeeze(1)], enforce_sorted=False)\n",
    "print(\"packed_input:\", packed_input)\n",
    "output, (h, c) = lstm(packed_input, (h0, c0))\n",
    "print(\"output:\", output.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def random_sample(batch_size):\n",
    "    line_tensors = []\n",
    "    label_tensors = []\n",
    "    for i in range(batch_size):\n",
    "        language = randomChoice(all_categories)  # sample a language\n",
    "        line = randomChoice(category_lines[language])  # sample from the target language\n",
    "        label_tensors.append(torch.tensor(all_categories.index(language), dtype=torch.long))\n",
    "        line_tensors.append(lineToTensor(line).squeeze(1).to(device))\n",
    "    return line_tensors, label_tensors\n",
    "        \n",
    "\n",
    "def train(model, n_hidden, criterion, n_iter, batch_size):\n",
    "    \n",
    "    losses = []\n",
    "    # init hidden and cell\n",
    "    h = torch.zeros(1, batch_size, n_hidden).to(device)\n",
    "    c = torch.zeros(1, batch_size, n_hidden).to(device)\n",
    "    \n",
    "    for it in range(n_iter):\n",
    "        model.zero_grad()\n",
    "        # randomly sample training data and the labels\n",
    "        line_tensors, label_tensors = random_sample(batch_size)\n",
    "        # feed to lstm\n",
    "        output, (h, c) = lstm(pack_sequence(line_tensors), (h, c))\n",
    "        print(output)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, label_tensors)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # step\n",
    "        for p in lstm.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "        # output and loss\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[-3.2806e-02,  1.8034e-02, -7.9790e-03, -2.0515e-03,  4.9728e-03,\n",
      "          1.7588e-02,  1.0416e-02,  2.3151e-02,  4.9440e-02, -3.8207e-02,\n",
      "         -9.2639e-03,  5.5615e-03,  4.9893e-02,  1.9164e-02, -2.4661e-02,\n",
      "         -8.1042e-04,  3.3157e-02, -2.3655e-02,  8.1463e-03, -1.5489e-02,\n",
      "          1.5290e-02,  3.1640e-02, -8.9267e-03,  1.3939e-02, -1.0499e-02,\n",
      "          1.1536e-02, -3.8022e-02, -2.3350e-03, -2.1018e-02,  1.5807e-03,\n",
      "         -6.5252e-03, -2.3276e-02,  4.9530e-03, -1.5157e-02,  2.4052e-02,\n",
      "         -3.4719e-02, -1.8018e-02,  2.0408e-02,  5.1119e-02,  1.1064e-02,\n",
      "         -4.8472e-02,  3.2297e-02, -1.0744e-02, -6.8858e-03,  2.7669e-02,\n",
      "         -2.3368e-03,  1.0422e-02, -2.1053e-02,  2.3932e-02,  1.4584e-02,\n",
      "          2.1264e-02, -2.5557e-02, -2.2317e-02,  1.8432e-02,  1.8573e-02,\n",
      "          4.6148e-02, -3.1822e-02,  1.4348e-03, -1.7959e-02, -7.8263e-03,\n",
      "         -2.3215e-02, -2.3061e-02,  5.5196e-03,  1.6678e-02,  1.5534e-02,\n",
      "          6.9284e-03,  4.4680e-02,  4.2447e-02, -1.7192e-02, -3.4980e-02,\n",
      "         -1.7411e-02,  1.2252e-03,  1.5046e-03,  4.7077e-03,  1.7494e-02,\n",
      "          5.3478e-02,  3.4434e-02,  2.4785e-03,  4.1875e-02, -2.5939e-02,\n",
      "         -1.2603e-02, -2.4767e-02,  4.2229e-02,  1.9539e-02, -2.9207e-02,\n",
      "          4.7765e-03, -1.0350e-02,  3.2996e-03,  4.7110e-03,  2.3879e-02,\n",
      "         -1.7966e-02, -2.3711e-02, -2.5124e-02, -1.0803e-02, -8.6460e-03,\n",
      "         -8.6323e-03,  4.0427e-02, -3.2341e-02, -2.9818e-02,  1.4120e-02,\n",
      "         -2.8470e-02,  2.0057e-03,  8.4915e-03,  1.2016e-02,  8.3690e-06,\n",
      "         -3.1930e-02, -1.7174e-02,  4.1077e-02, -2.2608e-02, -3.9055e-03,\n",
      "         -4.7987e-02, -1.8522e-02,  1.4708e-02, -1.7870e-02, -2.9085e-02,\n",
      "         -2.9531e-02,  3.9488e-03, -4.3974e-04,  2.0982e-02, -2.1206e-02,\n",
      "         -1.3512e-03, -4.5283e-03, -1.9938e-02, -2.5851e-02,  1.5222e-02,\n",
      "         -8.5705e-03,  2.3989e-02,  3.7148e-02],\n",
      "        [-3.6625e-02,  6.1519e-02, -2.1341e-02,  1.4536e-02, -2.2283e-02,\n",
      "          6.8717e-03,  8.4221e-03,  3.1869e-02,  6.2075e-02, -3.4675e-02,\n",
      "         -1.8630e-03, -8.4383e-03,  4.1035e-02,  3.0472e-02, -4.9815e-03,\n",
      "          2.0978e-02,  5.4414e-02, -5.7162e-02,  4.9729e-02, -2.4740e-02,\n",
      "          5.0662e-02,  4.9748e-02, -1.5986e-02,  2.6858e-02, -2.0794e-02,\n",
      "          1.5092e-02, -6.9427e-02, -1.8100e-02, -2.4830e-02, -1.2684e-03,\n",
      "         -2.3187e-02, -1.2495e-02, -8.3483e-03, -3.5603e-03,  4.3858e-02,\n",
      "         -2.7881e-02, -3.1088e-02,  2.1467e-02,  6.4268e-02,  2.9494e-02,\n",
      "         -6.5007e-02,  5.4208e-02, -1.8304e-02,  1.4435e-02,  3.1940e-03,\n",
      "          2.3704e-02,  1.8669e-03, -1.2198e-02,  1.3401e-02,  1.3659e-02,\n",
      "          8.1848e-03, -2.5718e-02, -4.7816e-02,  1.2354e-02,  2.2360e-03,\n",
      "          7.2855e-02, -3.7438e-02, -1.9796e-02, -1.8125e-02,  8.2641e-03,\n",
      "         -1.1213e-02, -2.7200e-03, -2.3678e-03,  3.3806e-02,  2.3400e-02,\n",
      "          2.9721e-02,  5.9802e-02,  2.4883e-02, -2.2119e-02, -4.4882e-02,\n",
      "          1.0102e-02,  2.2881e-02,  7.5908e-03, -1.7920e-02,  7.1894e-03,\n",
      "          6.0475e-02,  3.0371e-02, -1.0485e-02,  4.2936e-02, -4.6428e-02,\n",
      "         -3.6423e-02, -1.4550e-02,  2.9191e-02,  4.1021e-03, -4.8901e-02,\n",
      "          2.1718e-03, -2.5552e-02,  6.3921e-03, -9.4032e-03,  2.0300e-02,\n",
      "         -1.5042e-02, -1.4763e-02, -4.3452e-03,  3.8403e-03, -1.4712e-02,\n",
      "         -3.4281e-02,  4.1624e-02, -1.4178e-02, -3.0591e-02,  2.9917e-02,\n",
      "         -2.5205e-03, -5.7897e-03,  2.9749e-02,  4.5660e-02,  3.0571e-02,\n",
      "         -1.7167e-02, -2.3003e-02,  5.3087e-02, -1.6367e-02,  2.5552e-03,\n",
      "         -4.7761e-02, -2.6078e-02,  1.5968e-02, -2.9808e-02, -1.1312e-02,\n",
      "         -6.4876e-02,  7.3676e-04,  2.8607e-03, -5.2561e-04, -3.3968e-02,\n",
      "         -8.0253e-03, -1.0858e-02, -3.1227e-02, -5.7601e-02, -1.4583e-02,\n",
      "         -3.3393e-02,  1.6947e-02,  4.0064e-02],\n",
      "        [-4.2576e-02,  5.3146e-02, -3.0110e-02, -8.8060e-03, -2.9815e-02,\n",
      "          4.8602e-03,  3.2790e-02,  2.6981e-02,  5.6697e-02, -7.2228e-02,\n",
      "         -1.3836e-02, -2.8594e-02,  2.1984e-02,  2.7134e-02, -1.2851e-02,\n",
      "          3.9886e-02,  6.5030e-02, -5.4657e-02,  4.9228e-02, -5.5184e-03,\n",
      "          4.1577e-02,  3.4295e-02, -9.5426e-03,  2.3937e-03, -2.9243e-02,\n",
      "          8.5478e-03, -4.9450e-02, -2.8779e-02, -9.0354e-03, -1.1327e-02,\n",
      "         -3.0433e-02, -1.2940e-02,  8.2727e-03,  8.8805e-03,  7.3978e-02,\n",
      "         -3.5928e-02, -3.1531e-02,  1.7460e-02,  6.6538e-02,  3.8224e-02,\n",
      "         -4.0014e-02,  5.6434e-02, -3.2843e-02, -1.6119e-02,  1.9273e-04,\n",
      "          3.6829e-02,  1.5375e-02, -1.7265e-02,  3.4259e-03, -4.5464e-03,\n",
      "          2.0521e-02, -3.6069e-02, -5.0853e-02,  1.8599e-03,  1.8630e-02,\n",
      "          8.8515e-02, -5.2787e-02, -3.4493e-02, -3.1473e-02, -1.6727e-02,\n",
      "         -3.5450e-02, -1.9602e-02, -3.4930e-02,  4.5202e-02,  2.2286e-02,\n",
      "          4.5430e-02,  6.3043e-02,  2.1144e-02, -4.2405e-02, -5.0978e-02,\n",
      "         -1.4617e-02,  2.3865e-02,  1.2502e-02, -2.9913e-02,  8.5687e-03,\n",
      "          8.6209e-02,  2.6453e-02,  1.3627e-02,  3.8731e-02, -5.6993e-02,\n",
      "         -2.7211e-02, -3.8831e-02,  3.6848e-02,  1.4464e-02, -5.3155e-02,\n",
      "         -3.1335e-02, -1.0069e-02,  7.2716e-03, -3.8845e-02,  3.1776e-02,\n",
      "         -4.1031e-02, -7.0405e-04, -1.7085e-02,  1.5267e-02, -3.5814e-02,\n",
      "         -4.7978e-02,  1.8583e-02, -3.7910e-02, -4.7215e-02,  4.6421e-02,\n",
      "         -1.3093e-02, -2.9492e-02,  1.0306e-02,  5.2013e-02,  5.2292e-02,\n",
      "         -2.9097e-02, -1.9174e-02,  3.9188e-02, -1.1599e-02, -6.7057e-04,\n",
      "         -7.1749e-02, -3.2105e-02,  4.2219e-02, -4.6085e-02, -4.6735e-02,\n",
      "         -7.9666e-02, -8.8238e-03,  1.8721e-02,  1.4222e-02, -4.3862e-02,\n",
      "         -1.0215e-02, -8.4858e-03, -1.6567e-02, -6.9387e-02, -1.9717e-02,\n",
      "         -3.2056e-02,  3.5863e-02,  3.2622e-02],\n",
      "        [-4.3374e-02,  8.3420e-02, -2.7813e-02,  1.5242e-02, -4.0792e-02,\n",
      "          5.0240e-04,  1.7238e-02,  3.6110e-02,  6.4181e-02, -5.3298e-02,\n",
      "          1.5412e-04, -3.7839e-02,  2.3460e-02,  3.1037e-02, -4.0407e-03,\n",
      "          4.2783e-02,  7.6119e-02, -7.5550e-02,  6.9637e-02, -2.3704e-02,\n",
      "          7.2444e-02,  5.4513e-02, -1.3664e-02,  1.6879e-02, -2.7877e-02,\n",
      "          1.8393e-02, -7.6674e-02, -2.9342e-02, -1.5961e-02, -6.4989e-03,\n",
      "         -3.9170e-02, -3.9353e-03, -4.9495e-03,  1.0558e-02,  6.7827e-02,\n",
      "         -3.0158e-02, -3.2622e-02,  1.5635e-02,  8.2182e-02,  4.8230e-02,\n",
      "         -6.1664e-02,  7.0783e-02, -2.7186e-02,  4.6022e-03, -5.3073e-03,\n",
      "          4.7266e-02,  1.6505e-03, -1.0982e-02,  1.6059e-03,  6.3103e-03,\n",
      "          1.2591e-02, -3.0323e-02, -6.1305e-02,  7.9940e-03,  9.9198e-04,\n",
      "          9.9802e-02, -5.1474e-02, -3.0440e-02, -2.3160e-02,  6.9433e-03,\n",
      "         -1.6407e-02,  3.1456e-03, -2.0328e-02,  5.2297e-02,  2.6831e-02,\n",
      "          4.6924e-02,  7.6079e-02,  1.7859e-02, -3.1049e-02, -5.4739e-02,\n",
      "          8.9178e-03,  2.7643e-02,  1.2764e-02, -3.4588e-02,  2.0398e-03,\n",
      "          7.4405e-02,  2.5504e-02, -2.8455e-03,  4.7402e-02, -6.2007e-02,\n",
      "         -4.7560e-02, -2.2958e-02,  2.5407e-02, -2.9951e-03, -5.8956e-02,\n",
      "         -1.9127e-02, -2.6449e-02,  9.0660e-03, -2.9658e-02,  2.9260e-02,\n",
      "         -2.4286e-02, -7.5550e-03,  8.4128e-04,  1.8910e-02, -2.9250e-02,\n",
      "         -5.1252e-02,  2.4413e-02, -1.2547e-02, -3.8646e-02,  4.3071e-02,\n",
      "          8.8969e-03, -1.8191e-02,  2.6619e-02,  6.7031e-02,  5.7508e-02,\n",
      "         -1.4650e-02, -2.3708e-02,  5.3787e-02, -1.0398e-02,  8.2221e-03,\n",
      "         -5.8850e-02, -3.6788e-02,  2.9076e-02, -4.1324e-02, -2.5707e-02,\n",
      "         -9.0955e-02, -1.8813e-03,  9.2764e-03, -2.8261e-04, -4.5646e-02,\n",
      "         -1.6247e-02, -1.3252e-02, -3.0665e-02, -8.4472e-02, -3.0249e-02,\n",
      "         -4.7136e-02,  2.2060e-02,  3.3401e-02],\n",
      "        [-6.1113e-02,  5.5875e-02, -3.2804e-02,  1.1860e-02, -4.2423e-02,\n",
      "         -3.0848e-04,  5.1915e-03,  1.6547e-02,  4.2574e-02, -6.2033e-02,\n",
      "         -7.7829e-03, -7.4293e-02,  3.9498e-02,  1.1714e-02, -2.1906e-02,\n",
      "          2.7994e-02,  8.4799e-02, -5.4473e-02,  5.2956e-02, -9.0670e-03,\n",
      "          7.2597e-02,  7.3236e-02,  1.0663e-02,  3.3202e-03, -1.9446e-02,\n",
      "          1.4512e-02, -5.1286e-02, -4.8425e-02, -1.2543e-03,  2.8759e-04,\n",
      "         -3.3632e-02, -2.9396e-02, -2.5278e-03,  1.9872e-03,  4.9140e-02,\n",
      "         -6.2294e-02, -9.4336e-03,  2.8943e-02,  9.7721e-02,  3.2841e-02,\n",
      "         -7.8442e-02,  5.9554e-02, -2.6620e-02,  9.7753e-03,  1.9564e-02,\n",
      "          6.1017e-02, -8.9785e-03,  1.8263e-02,  2.2699e-02,  8.6911e-06,\n",
      "          3.2625e-02, -5.0762e-02, -3.0803e-02,  5.5104e-03,  1.1010e-02,\n",
      "          8.9076e-02, -2.0050e-02, -3.5818e-02, -8.2180e-03,  2.5040e-02,\n",
      "         -3.1448e-02,  1.3302e-02, -3.1849e-02,  3.5589e-02,  2.8139e-02,\n",
      "          3.9295e-02,  7.8500e-02,  2.2813e-02, -2.8540e-02, -5.7945e-02,\n",
      "          4.5292e-03, -5.3924e-03,  3.1613e-02, -3.5429e-02,  2.9031e-03,\n",
      "          8.5969e-02,  2.8182e-02,  7.7102e-03,  4.8067e-02, -3.5334e-02,\n",
      "         -5.4809e-02, -3.7026e-02,  4.6980e-02, -4.7967e-03, -6.7786e-02,\n",
      "         -4.6078e-02, -2.8974e-02, -1.5051e-02, -3.2017e-02,  4.3225e-02,\n",
      "         -1.5474e-02,  7.2941e-03, -3.5433e-03, -1.7860e-02, -3.7508e-02,\n",
      "         -4.2257e-02,  2.6383e-02, -3.3423e-02, -3.9128e-02,  3.7429e-02,\n",
      "          1.2013e-02,  4.8385e-03,  3.8148e-02,  4.2733e-02,  3.6573e-02,\n",
      "         -3.9609e-02, -3.9800e-02,  6.0991e-02,  3.3922e-03,  3.5127e-02,\n",
      "         -7.8718e-02, -5.7774e-02,  4.7969e-02, -2.7019e-02, -1.2017e-02,\n",
      "         -9.0824e-02, -1.2358e-02, -1.0586e-02,  4.4672e-03, -2.4127e-02,\n",
      "         -1.9890e-02, -3.7549e-02, -2.4088e-02, -9.9177e-02,  1.0612e-02,\n",
      "         -4.1224e-02,  1.7261e-02,  2.2810e-02],\n",
      "        [-5.2258e-02,  8.5526e-02, -2.9590e-02,  2.5587e-02, -4.7090e-02,\n",
      "         -8.6769e-03,  6.8640e-03,  3.0748e-02,  6.2061e-02, -5.0576e-02,\n",
      "          4.0798e-03, -5.9204e-02,  3.0915e-02,  2.5576e-02, -3.7071e-03,\n",
      "          3.7755e-02,  8.4055e-02, -7.4081e-02,  7.3043e-02, -2.8971e-02,\n",
      "          9.1238e-02,  7.3149e-02, -4.4454e-03,  1.6981e-02, -2.6434e-02,\n",
      "          2.0278e-02, -7.7645e-02, -3.9993e-02, -1.0568e-02,  1.3059e-03,\n",
      "         -3.8432e-02, -1.5203e-02, -1.6473e-02,  9.8925e-03,  6.1159e-02,\n",
      "         -4.2538e-02, -2.2985e-02,  2.2456e-02,  9.6874e-02,  4.4920e-02,\n",
      "         -7.9548e-02,  7.2241e-02, -2.7430e-02,  1.4792e-02,  1.1591e-03,\n",
      "          6.0843e-02, -1.3282e-02,  7.7884e-03,  1.0157e-02,  1.0776e-02,\n",
      "          2.0326e-02, -3.6343e-02, -5.2101e-02,  7.1541e-03, -8.8069e-04,\n",
      "          9.7280e-02, -3.0239e-02, -3.2534e-02, -1.2125e-02,  2.8276e-02,\n",
      "         -1.6111e-02,  2.2274e-02, -2.3374e-02,  4.3962e-02,  2.6133e-02,\n",
      "          4.1926e-02,  7.7288e-02,  1.6120e-02, -2.4327e-02, -5.6114e-02,\n",
      "          1.8399e-02,  1.8838e-02,  1.8294e-02, -3.9047e-02, -5.9531e-04,\n",
      "          7.5066e-02,  2.4247e-02, -4.0395e-03,  5.1003e-02, -5.3368e-02,\n",
      "         -5.9842e-02, -2.0524e-02,  3.2775e-02, -9.0336e-03, -5.9972e-02,\n",
      "         -3.0079e-02, -3.4826e-02, -6.4820e-03, -3.0465e-02,  3.4184e-02,\n",
      "         -1.5805e-02, -8.2000e-03,  9.4115e-03, -1.8288e-04, -3.1599e-02,\n",
      "         -4.9178e-02,  2.5496e-02, -1.0766e-02, -3.1510e-02,  3.9042e-02,\n",
      "          1.9075e-02,  1.2935e-04,  3.9369e-02,  6.4292e-02,  5.3796e-02,\n",
      "         -1.9179e-02, -3.2314e-02,  6.1819e-02, -5.2266e-03,  2.3947e-02,\n",
      "         -6.0912e-02, -5.0114e-02,  3.1123e-02, -3.6317e-02, -9.1521e-03,\n",
      "         -9.5784e-02, -6.7254e-03, -4.6167e-03, -4.4432e-03, -3.5919e-02,\n",
      "         -1.8491e-02, -2.7949e-02, -3.2666e-02, -1.0581e-01, -1.8248e-02,\n",
      "         -5.1323e-02,  1.1429e-02,  2.8183e-02]], device='cuda:0',\n",
      "       grad_fn=<CudnnRnnBackward>), batch_sizes=tensor([1, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PackedSequence' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-4105a7092e56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_letters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-c6e3f1389c1d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, n_hidden, criterion, n_iter, batch_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2107\u001b[0;31m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected 2 or more dimensions (got {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PackedSequence' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "n_iter = 1\n",
    "batch_size = 1\n",
    "lstm = nn.LSTM(input_size=n_letters, hidden_size=n_hidden).to(device)\n",
    "\n",
    "train(lstm, n_hidden, criterion, n_iter, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

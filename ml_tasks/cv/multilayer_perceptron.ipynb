{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "Multilayer Perceptron (MLP) is probably the simplist form of neuroal networks. It is mainly used for classification problem where there is stron non-linearality among features. A MLP consists of an input layer, one or more hidden layers (including the output layer), each of which is a full connected layer. The output of each layer goes through an activation function (usually ReLu) before going to the next layer.\n",
    "\n",
    "![MLP](./imgs/multilayer_perceptron.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, ndim, hidden_layer_dims):\n",
    "        self.all_dims = [ndim] + hidden_layer_dims\n",
    "        np.random.seed(99)\n",
    "        self.parameters = {}\n",
    "        for layer in range(len(self.all_dims) - 1):\n",
    "            self.parameters['W%d' % layer] = np.random.randn(self.all_dims[layer + 1], self.all_dims[layer]) * 0.1\n",
    "            self.parameters['b%d' % layer] = np.random.randn(self.all_dims[layer + 1], 1) * 0.1\n",
    "        \n",
    "    def relu_(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_backward_(self, dActivation, z):\n",
    "        dz = np.array(dActivation, copy=True)\n",
    "        dz[z <= 0] = 0\n",
    "        dz[z > 0] = 1\n",
    "        return dz\n",
    "    \n",
    "    def forward_one_layer_(self, activation_prev, W, b):\n",
    "        z = W.dot(activation_prev) + b\n",
    "#         print('W.shape:', W.shape, 'a_prev.shape:', activation_prev.shape, 'b.shape:', b.shape, 'z.shape:', z.shape)\n",
    "        return self.relu_(z), z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Conduct feed forward for the NN.\"\"\"\n",
    "        activation_cur = np.expand_dims(x, axis=1)\n",
    "        intermediate_results = {}  # store the intermediate results for backpropagation\n",
    "        \n",
    "        for i in range(len(self.all_dims) - 1):\n",
    "            W = self.parameters['W%d' % i]\n",
    "            b = self.parameters['b%d' % i]\n",
    "            activation_prev = activation_cur\n",
    "            activation_cur, z_cur = self.forward_one_layer_(activation_prev, W, b)\n",
    "            intermediate_results['a%d' % i] = activation_prev\n",
    "            intermediate_results['Z%d' % i] = z_cur\n",
    "        \n",
    "        return activation_cur, intermediate_results\n",
    "    \n",
    "    def cost_(self, y, hat_y):\n",
    "        \"\"\"Cross-entropy cost\"\"\"\n",
    "        n = hat_y.shape[1]\n",
    "        print('y:', y, 'hat_y:', hat_y)\n",
    "        print(np.dot(y, np.log(hat_y)), np.dot(1 - y, np.log(1 - hat_y)))\n",
    "        cost = -1 / n * (np.dot(y, np.log(hat_y)) + np.dot(1 - y, np.log(1 - hat_y)))\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def backward_propogation_one_layer_(self, dA_cur, W, b, z, acivation_prev):\n",
    "        n = activation_prev.shape[1]\n",
    "        \n",
    "        dz = self.relu_backward_(dA_cur, z)  # dZ = dA * g'(z)\n",
    "        \n",
    "        dW = dZ_curr.dot(activation_prev.T) / n  # dW = 1/n * dZ * a_prev\n",
    "        db = np.sum(dz, axis=1, keepdims=True) / n  # db = 1/n * \\sum_i^n dz\n",
    "        da_prev = W.T.dot(dz)  # da = W^T dZ\n",
    "        \n",
    "        return da_prev, dW, db\n",
    "\n",
    "    def backward_propagation(self, hat_y, y, intermediate_results):\n",
    "        n = y.shape[1]\n",
    "        y = y.reshape(hat_y.shape)\n",
    "        grads = {}\n",
    "        \n",
    "        da_prev = -(np.divide(y, y_hat))  # dcross_entropy/da\n",
    "        \n",
    "        for layer in reversed(len(self.all_dims) - 1):\n",
    "            da_cur = da_prev\n",
    "            \n",
    "            a_prev = intermediate_results['a%d' % layer]\n",
    "            z_cur = intermediate_results['z%d' % layer]\n",
    "            W_cur = self.parameters['W%d' % layer]\n",
    "            b_cur = self.parameters['b%d' % layer]\n",
    "            \n",
    "            da_prev, dW, db = self.backward_propogation_one_layer_(da_cur, W, b, z, a_prev)\n",
    "            grads['dW%d' % layer] = dW\n",
    "            grads['db%d' % layer] = db\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    def update(self, grads, lr):\n",
    "        for layer in range(len(self.all_dims)):\n",
    "            self.parameters['W%d' % layer] -= learning_rate * grads['dW%d' % layer]\n",
    "            self.parameters['b%d' % layer] -= learning_rate * grads['db%d' % layer]\n",
    "\n",
    "    def train(self, X, y, epochs, lr):\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            for x, y in zip(X, y):\n",
    "                hat_y, intermediate_results = self.forward(x)\n",
    "                cost = self.cost_(y, hat_y)\n",
    "                print(cost)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0 (8, 15)\n",
      "b0 (8, 1)\n",
      "W1 (2, 8)\n",
      "b1 (2, 1)\n",
      "y: [[0 1]] hat_y: [[0.]\n",
      " [0.]]\n",
      "[[nan]] [[0.]]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxjiang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:45: RuntimeWarning: divide by zero encountered in log\n",
      "/home/yxjiang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:46: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "n_dim = 15\n",
    "hidden_layer_dims = [8, 2]\n",
    "\n",
    "model = MultiLayerPerceptron(n_dim, hidden_layer_dims)\n",
    "for name in model.parameters:\n",
    "    print(name, model.parameters[name].shape)\n",
    "    \n",
    "X = np.random.randn(n, n_dim)\n",
    "y = np.array([[[1, 0]] if r >= 2 else [[0, 1]] for r in np.sum(X, axis=1)])  # dim: (n, n_out)\n",
    "\n",
    "model.train(X, y, epochs=1, lr=0.01)\n",
    "# for it in range(1):\n",
    "#     X = np.random.randn(n_batch, n_dim)\n",
    "#     y = np.array([[1, 0] if r >= 2 else [0, 1] for r in np.sum(X, axis=1)])  # dim: (n_batch, n_out)\n",
    "#     loss = model.train(X, y, lr=0.001)\n",
    "#     if it % 100 == 0:\n",
    "#         print('iteration %d, loss: %.3f' % (it, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    dZ[Z > 0] = 1\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, nn_architecture):\n",
    "        self.nn_architecture = nn_architecture\n",
    "        number_of_layers = len(nn_architecture)\n",
    "        self.params_values = {}\n",
    "\n",
    "        for idx, layer in enumerate(nn_architecture):\n",
    "            layer_idx = idx + 1\n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "            self.params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "                layer_output_size, layer_input_size) * 0.1\n",
    "            self.params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "                layer_output_size, 1) * 0.1\n",
    "    \n",
    "    def single_layer_forward_propagation(self, A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "        Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "        \n",
    "        if activation is \"relu\":\n",
    "            activation_func = relu\n",
    "        elif activation is \"sigmoid\":\n",
    "            activation_func = sigmoid\n",
    "        else:\n",
    "            raise Exception('Non-supported activation function')\n",
    "\n",
    "        return activation_func(Z_curr), Z_curr\n",
    "    \n",
    "    def full_forward_propagation(self, X):\n",
    "        memory = {}\n",
    "        A_curr = X\n",
    "\n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            layer_idx = idx + 1\n",
    "            A_prev = A_curr\n",
    "            \n",
    "            activ_function_curr = layer[\"activation\"]\n",
    "            W_curr = self.params_values[\"W\" + str(layer_idx)]\n",
    "            b_curr = self.params_values[\"b\" + str(layer_idx)]\n",
    "            A_curr, Z_curr = self.single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "\n",
    "            memory[\"A\" + str(idx)] = A_prev\n",
    "            memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "\n",
    "        return A_curr, memory\n",
    "    \n",
    "    def get_cost_value(self, Y_hat, Y):\n",
    "        m = Y_hat.shape[1]\n",
    "        cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def single_layer_backward_propagation(self, dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        if activation is \"relu\":\n",
    "            backward_activation_func = relu_backward\n",
    "        elif activation is \"sigmoid\":\n",
    "            backward_activation_func = sigmoid_backward\n",
    "        else:\n",
    "            raise Exception('Non-supported activation function')\n",
    "\n",
    "        dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "        dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "        dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "        return dA_prev, dW_curr, db_curr\n",
    "    \n",
    "    def full_backward_propagation(self, Y_hat, Y, memory):\n",
    "        grads_values = {}\n",
    "        m = Y.shape[1]\n",
    "        Y = Y.reshape(Y_hat.shape)\n",
    "\n",
    "        dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "\n",
    "        for layer_idx_prev, layer in reversed(list(enumerate(self.nn_architecture))):\n",
    "            layer_idx_curr = layer_idx_prev + 1\n",
    "            activ_function_curr = layer[\"activation\"]\n",
    "\n",
    "            dA_curr = dA_prev\n",
    "\n",
    "            A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "            Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "            W_curr = self.params_values[\"W\" + str(layer_idx_curr)]\n",
    "            b_curr = self.params_values[\"b\" + str(layer_idx_curr)]\n",
    "\n",
    "            dA_prev, dW_curr, db_curr = self.single_layer_backward_propagation(\n",
    "                dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "\n",
    "            grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "            grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "\n",
    "        return grads_values\n",
    "    \n",
    "    def update(self, grads_values, learning_rate):\n",
    "        for layer_idx, layer in enumerate(self.nn_architecture, 1):\n",
    "            self.params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "            self.params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate):\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "\n",
    "        y_hat, intermediate_results = self.full_forward_propagation(X)\n",
    "        cost = self.get_cost_value(y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "#             accuracy = get_accuracy_value(Y_hat, Y)\n",
    "#             accuracy_history.append(accuracy)\n",
    "\n",
    "        grads_values = self.full_backward_propagation(y_hat, Y, intermediate_results)\n",
    "        params_values = self.update(grads_values, learning_rate)\n",
    "\n",
    "        return cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007579618537019549\n",
      "0.6529857393631405\n",
      "0.6780528066303901\n",
      "0.686477591477436\n",
      "0.6828785042141535\n",
      "0.6699286410439346\n",
      "0.6854692835061437\n",
      "0.6652953669701144\n",
      "0.6612776896652122\n",
      "0.6606872294896019\n",
      "0.6814804216216969\n",
      "0.6944806579815238\n",
      "0.6794791658753289\n",
      "0.6782283572492398\n",
      "0.6781284073242644\n",
      "0.695739943626792\n",
      "0.6686272252631638\n",
      "0.6849503555637149\n",
      "0.660429481766363\n",
      "0.7048727498586271\n",
      "0.682716737793009\n",
      "0.6611083153385592\n",
      "0.6664825431505288\n",
      "0.6742044022157093\n",
      "0.667138656970784\n",
      "0.6603844847170635\n",
      "0.6384840217659447\n",
      "0.6699391853618364\n",
      "0.6699261512989881\n",
      "0.6973432399108916\n",
      "0.6908949008786938\n",
      "0.6622894638516514\n",
      "0.6615805604230117\n",
      "0.6974290400074409\n",
      "0.6601696782383679\n",
      "0.6786233615497059\n",
      "0.6791847421649259\n",
      "0.6995592290666373\n",
      "0.6818321985873607\n",
      "0.6781216127526569\n",
      "0.6818970272789671\n",
      "0.6653213318149527\n",
      "0.6700130853672326\n",
      "0.6652306118323053\n",
      "0.6969271653236394\n",
      "0.678106815990152\n",
      "0.6401046959902995\n",
      "0.6422475141272203\n",
      "0.6699275836372476\n",
      "0.6744657949771671\n",
      "0.6794623876878516\n",
      "0.6615314383724106\n",
      "0.716295000377829\n",
      "0.6780719641134084\n",
      "0.6914135729143905\n",
      "0.707266549464631\n",
      "0.6668200609303\n",
      "0.6907794840997211\n",
      "0.6995857738870529\n",
      "0.6952529477704203\n",
      "0.6552161774392745\n",
      "0.6352221225422028\n",
      "0.6628243623254677\n",
      "0.6876570923460144\n",
      "0.6394807433074444\n",
      "0.6437638778390609\n",
      "0.6906451037063218\n",
      "0.674464865352695\n",
      "0.7088915778248588\n",
      "0.65923397419591\n",
      "0.6605321565832079\n",
      "0.6780494985381044\n",
      "0.6817404470679982\n",
      "0.6700285820097305\n",
      "0.6373373103997232\n",
      "0.6504644541523585\n",
      "0.6742061832183271\n",
      "0.6570393211186566\n",
      "0.6662612986953326\n",
      "0.6701521542548725\n",
      "0.6606066135350378\n",
      "0.6999784672183202\n",
      "0.6970393291120182\n",
      "0.6742476221643429\n",
      "0.6754207410277615\n",
      "0.6663367859753987\n",
      "0.6552774746992134\n",
      "0.6749821095581429\n",
      "0.6824022651822149\n",
      "0.666138961210206\n",
      "0.7020352418568667\n",
      "0.6939247387217375\n",
      "0.7040338063737106\n",
      "0.6545717650631798\n",
      "0.6507022578874891\n",
      "0.6840649100537257\n",
      "0.7097570723847751\n",
      "0.6456856931431959\n",
      "0.6713812541891876\n",
      "0.6871116988337788\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "\n",
    "nn_architecture = [\n",
    "    {\"input_dim\": 15, \"output_dim\": 8, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 8, \"output_dim\": 5, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 5, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "model = MLP(nn_architecture)\n",
    "\n",
    "losses = []\n",
    "for i in range(1000000):\n",
    "    X = np.random.randn(n, n_dim).T\n",
    "    y = np.array([[1.0] if r >= 1 else [0.0] for r in np.sum(X, axis=0)])\n",
    "    loss = model.train(X, y, learning_rate=0.01)\n",
    "    losses.append(loss[0])\n",
    "    if i % 10000 == 0:\n",
    "        print(sum(losses[-100:]) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

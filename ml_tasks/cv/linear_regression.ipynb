{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a predictive model that assumes the outcome has the linear relationship with the observed signals, e.g. $\\hat{y} = w^Tx$, where $\\hat{y}$ is the outcome predicted with respect to the given observation vector $x = (x_1, x_2, ..., x_k)$ and the learned weights of the model $w = (w_1, w_2, ..., w_k)$.\n",
    "\n",
    "![linear regression](./imgs/linear_regression.png)\n",
    "\n",
    "To quantify the predictability of the model, min-squared error (MSE) is used as the cost function is defined as $C = -\\frac{1}{2n}\\sum_m (y^{(m)} - \\hat{y^{(m)}})^2$. Usually, a regularization term $b = \\lambda ||w||_m$ is added to make less likely to overfit to the training data, making the cost function. Therefore the cost function is $C = \\frac{1}{2n} \\sum_m (y^{(m)} - \\hat{y^{(m)}}) + \\frac{\\lambda}{2}||w||_2^2$, and the partial derivative for each weight $w_i$ is $-(y - \\hat{y})x_i + \\lambda w_i$. If the weight update is conducted in mini-batch, the weight update for $w_i$ would be $-\\sum_m (y^{(m)} - \\hat{y^{(m)}})x_i + \\lambda\\sum_m w_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    \n",
    "    def __init__(self, ndim, l2_weight):\n",
    "        self.W = np.random.randn(ndim + 1, 1)  # to include the weight for the bias term\n",
    "        self.l2_weight = l2_weight\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict given a batch of inputs.\"\"\"\n",
    "        bias = np.ones((X.shape[0], 1))  # pretend 1 to X as the bias term\n",
    "        X = np.concatenate((bias, X), axis=1)\n",
    "        return self.W.T.dot(X.T).T  # y = w^T * x, dim: (n_batch, 1)\n",
    "    \n",
    "    def train(self, X, y, lr):\n",
    "        \"\"\" Update the model weights given a batch of training instances.\"\"\"\n",
    "        outputs = self.predict(X)  # dim (n_batch, 1)\n",
    "        pred_diffs = -(np.expand_dims(y, axis=1) - outputs)  # dim: (n_batch, 1)\n",
    "        bias = np.ones((X.shape[0], 1))  # pretend 1 to each input vector as the bias term\n",
    "        X_with_bias = np.concatenate((bias, X), axis=1)  # dim: (n_batch, ndim+1)\n",
    "        dW = np.sum(pred_diffs * X_with_bias + self.l2_weight * self.W.T, axis=0)  # (y-\\hat{y})*x_i\n",
    "        self.W -= lr * np.expand_dims(dW, axis=1) \n",
    "        return abs(np.sum(pred_diffs) / len(pred_diffs))  # return the loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.predict: [[-4.14617347]\n",
      " [-1.56619301]]\n",
      "iteration 1, loss: 0.47604\n",
      "iteration 11, loss: 0.59036\n",
      "iteration 21, loss: 0.28405\n",
      "iteration 31, loss: 0.05715\n",
      "iteration 41, loss: 0.01114\n",
      "iteration 51, loss: 0.01075\n",
      "iteration 61, loss: 0.00456\n",
      "iteration 71, loss: 0.01138\n",
      "iteration 81, loss: 0.00357\n",
      "iteration 91, loss: 0.00009\n"
     ]
    }
   ],
   "source": [
    "n_dim, n_batch = 15, 10\n",
    "\n",
    "model = LinearRegression(ndim=n_dim, l2_weight=0.01)\n",
    "X = np.random.randn(2, 15)\n",
    "print(\"model.predict:\", model.predict(X))\n",
    "\n",
    "# train the model to predict sum(x).\n",
    "for it in range(100):\n",
    "    X = np.random.randn(n_batch, n_dim)\n",
    "    y = np.sum(X, axis=1)\n",
    "    loss = model.train(X, y, 0.01)\n",
    "    if it % 10 == 0:\n",
    "        test_X = np.random.randn(1, n_dim)\n",
    "        test_y = np.sum(test_X, axis=1)\n",
    "        pred_y = model.predict(test_X)\n",
    "        print(\"iteration %d, loss: %.5f\" % (it + 1, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
